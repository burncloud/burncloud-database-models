use burncloud_database_models::{ModelDatabase, ModelInfo};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºæ•°æ®åº“å®ä¾‹
    let db = ModelDatabase::new().await?;

    // åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ¨¡å‹
    let model = ModelInfo {
        model_id: "openai-community/gpt4".to_string(),
        private: false,
        pipeline_tag: Some("text-generation".to_string()),
        library_name: Some("transformers".to_string()),
        model_type: Some("gpt2".to_string()),
        downloads: 1000000,
        likes: 5000,
        sha: Some("abc123".to_string()),
        last_modified: Some("2024-01-01 12:00:00".to_string()),
        gated: false,
        disabled: false,
        tags: r#"["transformers", "pytorch", "text-generation", "gpt2"]"#.to_string(),
        config: r#"{"architectures": ["GPT2LMHeadModel"], "model_type": "gpt2"}"#.to_string(),
        widget_data: "[]".to_string(),
        card_data: "{}".to_string(),
        transformers_info: "{}".to_string(),
        siblings: r#"[{"rfilename": "config.json"}, {"rfilename": "model.safetensors"}]"#.to_string(),
        spaces: "[]".to_string(),
        safetensors: "{}".to_string(),
        used_storage: 548000000,
        filename: Some("model.safetensors".to_string()),
        size: 548000000,
        created_at: "2024-01-01 10:00:00".to_string(),
        updated_at: "2024-01-01 12:00:00".to_string(),
    };

    // æ·»åŠ æ¨¡å‹
    db.add_model(&model).await?;
    println!("âœ… æ¨¡å‹æ·»åŠ æˆåŠŸ");

    // è·å–æ¨¡å‹
    if let Some(retrieved_model) = db.get_model("openai-community/gpt3").await? {
        println!("ğŸ“‹ è·å–åˆ°æ¨¡å‹: {}", retrieved_model.model_id);
        println!("   ç®¡é“ç±»å‹: {:?}", retrieved_model.pipeline_tag);
        println!("   ä¸‹è½½é‡: {}", retrieved_model.downloads);
        println!("   æ–‡ä»¶å: {:?}", retrieved_model.filename);
        println!("   æ–‡ä»¶å¤§å°: {} å­—èŠ‚", retrieved_model.size);
    }

    // è·å–æ‰€æœ‰æ¨¡å‹
    let all_models = db.list_models().await?;
    println!("ğŸ“Š æ€»å…±æœ‰ {} ä¸ªæ¨¡å‹", all_models.len());

    // æŒ‰ç®¡é“ç±»å‹æœç´¢
    let text_gen_models = db.search_by_pipeline("text-generation").await?;
    println!("ğŸ” æ‰¾åˆ° {} ä¸ªæ–‡æœ¬ç”Ÿæˆæ¨¡å‹", text_gen_models.len());

    // è·å–çƒ­é—¨æ¨¡å‹
    let popular_models = db.get_popular_models(5).await?;
    println!("ğŸ”¥ çƒ­é—¨æ¨¡å‹å‰5å:");
    for (i, model) in popular_models.iter().enumerate() {
        println!("   {}. {} (ä¸‹è½½é‡: {})", i + 1, model.model_id, model.downloads);
    }

    // å…³é—­æ•°æ®åº“
    db.close().await?;
    println!("ğŸ”’ æ•°æ®åº“è¿æ¥å·²å…³é—­");

    Ok(())
}